{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Heterogeneous Database Simulations\n",
    "\n",
    "Let's start generating random records and semantic operations, which will be used to execute performance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from database_generator import DatabaseGenerator\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Phase\n",
    "\n",
    "Inserting all records generated in the semantic heterogeneous database. Please note PyMongo library may diminish performance of insertions. However, because the simulator internally uses it, it is fair to also use it on our baseline test, so these delays might net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide all variables for this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_records = 100000\n",
    "number_of_versions = 5\n",
    "number_of_fields = 11\n",
    "number_of_values_in_domain=20\n",
    "\n",
    "number_of_tests = 5\n",
    "confidence_interval = 0.95\n",
    "\n",
    "host = 'localhost'\n",
    "\n",
    "performance_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First scenario\n",
    "Inserting all records and adding the semantic operations afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = first_scenario()\n",
    "    performance_results = performance_results.append({'stage' : 'LoadingPhase', 'experiment': 'insert-first','time':time_taken}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Scenario\n",
    "Loading records only after inserting semantic operations in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records.head(10), 'valid_from_date') #initial insert\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:          \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "\n",
    "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/Collection.py:165: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dates = dates.append([{'version_valid_from':datetime(2200,12,31)}])\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/Collection.py:165: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dates = dates.append([{'version_valid_from':datetime(2200,12,31)}])\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: '_pydevd_frame_eval.pydevd_frame_evaluator.get_bytecode_while_frame_eval_38'\n",
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_frame_eval/pydevd_frame_evaluator.pyx\", line 258, in _pydevd_frame_eval.pydevd_frame_evaluator.get_func_code_info\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 885, in get_abs_path_real_path_and_base_from_frame\n",
      "    ret = get_abs_path_real_path_and_base_from_file(f)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 852, in get_abs_path_real_path_and_base_from_file\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_83004/1775936195.py\", line 2, in <module>\n",
      "    time_taken = second_scenario()\n",
      "  File \"/tmp/ipykernel_83004/1279181990.py\", line 15, in second_scenario\n",
      "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/BasicCollection.py\", line 21, in insert_many_by_dataframe\n",
      "    self.collection.insert_many_by_dataframe(dataframe, ValidFromField)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/Collection.py\", line 189, in insert_many_by_dataframe\n",
      "    self.__insert_many_by_version(r_2[cols])\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/Collection.py\", line 236, in __insert_many_by_version\n",
      "    altered = self.semantic_operations[operationType].evolute_backward(v[1], v[0])\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/TranslationOperation.py\", line 321, in evolute_backward\n",
      "    if Document[operation['previous_operation.field'].values[0]] == operation['previous_operation.from'].values[0]:\n",
      "TypeError: string indices must be integers\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_frame_eval/pydevd_frame_evaluator.pyx\", line 256, in _pydevd_frame_eval.pydevd_frame_evaluator.get_func_code_info\n",
      "KeyError: '/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/pygments/formatter.py'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-pack    abs_path, canonical_normalized_filename = _abs_and_canonical_path(f)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 411, in _abs_and_canonical_path\n",
      "ages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 867, in get_abs_path_real_path_and_base_from_frame\n",
      "    return NORM_PATHS_AND_BASE_CONTAINER[frame.f_code.co_filename]\n",
      "KeyError: '/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/pygments/formatter.py'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 831, in get_abs_path_real_path_and_base_from_file\n",
      "    return NORM_PATHS_AND_BASE_CONTAINER[filename]\n",
      "KeyError: '/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/pygments/formatter.py'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pyd    real_path = _apply_func_and_normalize_case(filename, os_path_real_path, isabs, normalize)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 438, in _apply_func_and_normalize_case\n",
      "evd_file_utils.py\", line 385, in _abs_and_canonical_path\n",
      "    return NORM_PATHS_CONTAINER[filename]\n",
      "KeyError: '/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/pygments/formatter.py'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_frame_eval/pydevd_frame_evaluator.pyx\", line 258, in _pydevd_frame_eval.pydevd_frame_evaluator.get_func_code_info\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 885, in get_abs_path_real_path_and_base_from_frame\n",
      "    ret = get_abs_path_real_path_and_base_from_file(f)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 852, in get_abs_path_real_path_and_base_from_file\n",
      "    abs_path, canonical_normalized_filename = _abs_and_canonical_path(f)\n",
      "  File \"/home/pedro/    r = func(filename)\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 411, in _abs_and_canonical_path\n",
      "    real_path = _apply_func_and_normalize_case(filename, os_path_real_path, isabs, normalize)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py\", line 438, in _apply_func_and_normalize_case\n",
      "    r = func(filename)\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_83004/1775936195.py\", line 2, in <module>\n",
      "    time_taken = second_scenario()\n",
      "  File \"/tmp/ipykernel_83004/1279181990.py\", line 15, in second_scenario\n",
      "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/BasicCollection.py\", line 21, in insert_many_by_dataframe\n",
      "    self.collection.insert_many_by_dataframe(dataframe, ValidFromField)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/Collection.py\", line 189, in insert_many_by_dataframe\n",
      "    self.__insert_many_by_version(r_2[cols])\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/Collection.py\", line 236, in __insert_many_by_version\n",
      "    altered = self.semantic_operations[operationType].evolute_backward(v[1], v[0])\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongodb_extension/semantic_heterogeneous_database/TranslationOperation.py\", line 321, in evolute_backward\n",
      "    if Document[operation['previous_operation.field'].values[0]] == operation['previous_operation.from'].values[0]:\n",
      "TypeError: string indices must be integers\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 793, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 840, in get_records\n",
      "    formatter = Terminal256Formatter(style=style)\n",
      "  File \"/home/pedro/Documents/USP/Mestrado/Pesquisa/mongotest-venv/lib/python3.8/site-packages/pygments/formatters/terminal256.py\", line 136, in __init__\n",
      "    Formatter.__init__(self, **options)\n",
      "SystemError: <function Formatter.__init__ at 0x7fb571113c10> returned NULL without setting an error\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = second_scenario()\n",
    "    performance_results = performance_results.append({'stage' : 'LoadingPhase', 'experiment': 'operations-first','time':time_taken}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Scenario\n",
    "\n",
    "In a common database scenario, the records would be just inserted as they were generated (in raw format). User would have to deal with heterogeneity afterwards, in the querying fase. Therefore, for the loading phase, only generate raw records and bulk insert into the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    client = MongoClient(host)\n",
    "    client[d.database_name][d.collection_name].insert_many(d.records)\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = third_scenario()\n",
    "    performance_results = performance_results.append({'stage' : 'LoadingPhase', 'experiment': 'baseline','time':time_taken}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying tests\n",
    "\n",
    "For the query tests, it does not matter which database (from first loading phase or from the second) is used. Both of them posess the same number of records, fields and domain values. Let's now analyse statistics in six different scenarios, just as in YCDB benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_database_preinsert():\n",
    "    ### Generate database just as before\n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    \n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])  \n",
    "       \n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correctly compare performance between the developed system and the baseline (an ordinary document-oriented database), this test performs a query rewriting before starting the time counter. It is important to notice, however, this test compares time performance only, regardless of usability gains achieved by this system. This query rewriting would have to be manually performed by the user, which would potentially also spend time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_baseline(query, database_generator):\n",
    "    ors = list()\n",
    "\n",
    "    for operation in database_generator:\n",
    "        operationType, version_date, arguments = operation\n",
    "\n",
    "        if operationType == 'grouping':\n",
    "            pass\n",
    "        elif operationType == 'translation':\n",
    "            oldValue = arguments['from']\n",
    "            newValue = arguments['to']\n",
    "            field = arguments['field']\n",
    "\n",
    "            if field not in query:\n",
    "                continue\n",
    "            else:\n",
    "                ands = [{field : newValue}]\n",
    "                \n",
    "\n",
    "        else:\n",
    "            raise BaseException('OperationType unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_database_postinsert():\n",
    "    ### Generate database just as before\n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    \n",
    "    d.collection.insert_many_by_dataframe(records.head(10), 'valid_from_date') #initial insert\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:          \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "\n",
    "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')    \n",
    "    \n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_read_test(percent_of_update):\n",
    "    ### Generate database just as before    \n",
    "    d = generate_test_database_preinsert()   \n",
    "    original_records = d.records.copy()\n",
    "\n",
    "    updates = math.floor(100*percent_of_update)\n",
    "    reads = 100-updates\n",
    "\n",
    "    sequence = ([True]*updates)\n",
    "    sequence.extend([False]*reads)\n",
    "    random.shuffle(sequence)    \n",
    "\n",
    "    records = [d.generate_record() for i in range(updates)]\n",
    "    records_2 = records.copy()\n",
    "\n",
    "    queries = []   \n",
    "\n",
    "    for i in range(reads):        \n",
    "        field = (random.choice(d.fields))[0]\n",
    "        value = random.choice(d.field_domain[field])\n",
    "        queries.append({field:value})   \n",
    "\n",
    "    queries_2 = queries.copy()     \n",
    "\n",
    "    start = time.time()\n",
    "    for operation in sequence:             \n",
    "        if operation: # insert - Nos nossos casos de uso não faz muito sentido deleções e updates. \n",
    "            record = records.pop()\n",
    "            start_2 = time.time()\n",
    "            d.collection.insert_one(json.dumps(record, default=str),record['valid_from_date'])                        \n",
    "            end_2 = time.time()\n",
    "            #print('Insertion time:' + str(end_2-start_2))\n",
    "        else:\n",
    "            start_2 = time.time()\n",
    "            d.collection.find_many(queries.pop()) ##Nao to considerando a presença ou ausência de índices\n",
    "            end_2 = time.time()\n",
    "            #print('Query time:' + str(end_2-start_2))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Operations time: \"+ str(end-start))\n",
    "    ##Evolute everything in the end            \n",
    "        \n",
    "    end = time.time()\n",
    "    preinsered_time = end-start\n",
    "    d.destroy()\n",
    "\n",
    "    client = MongoClient(host)        \n",
    "    db = client[d.database_name]\n",
    "    base_collection = db[d.collection_name]\n",
    "\n",
    "    base_collection.insert_many(original_records)\n",
    "\n",
    "    start = time.time()    \n",
    "    for operation in sequence:             \n",
    "        if operation: \n",
    "            record = records_2.pop()\n",
    "            base_collection.insert_one(record)                      \n",
    "        else:\n",
    "            base_collection.find(queries_2.pop()) ##Isso nao faz exatamente sentido. Deveria gerar uma nova query \n",
    "    end = time.time()    \n",
    "    baseline_time = (end-start)\n",
    "    client.drop_database(d.database_name)\n",
    "    \n",
    "    return ({'preinserted': preinsered_time, 'baseline': baseline_time})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 0 - Full Insertion - Lazy insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(1)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 0 - Full Insertion - Not lazy insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(1,False)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion Nonlazy', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion Nonlazy', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1 - 50/50\n",
    "\n",
    "In the first update scenario, a workload of 50% of reads and 50% of writes.\n",
    "\n",
    "Note there is a difference between the test using the first generation method and the second one, due to lazy records evolution adopted in the prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(0.5)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Insertion (no query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(1)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2 - Read Heavy\n",
    "\n",
    "In this scenario, 95% of operations executed are read operations, while 5% are write operations (insertion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(0.05)\n",
    "    performance_results = performance_results.append({'stage' : 'ReadHeavy', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'ReadHeavy', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3 - WriteHeavy\n",
    "\n",
    "In this scenario, 95% of operations executed are write operations, while 5% are read operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(0.95)\n",
    "    performance_results = performance_results.append({'stage' : 'WriteHeavy', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'WriteHeavy', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongotestkernel",
   "language": "python",
   "name": "mongotestkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
