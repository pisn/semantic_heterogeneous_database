{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Heterogeneous Database Simulations\n",
    "\n",
    "Let's start generating random records and semantic operations, which will be used to execute performance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from database_generator import DatabaseGenerator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Phase\n",
    "\n",
    "Inserting all records generated in the semantic heterogeneous database. Please note PyMongo library may diminish performance of insertions. However, because the simulator internally uses it, it is fair to also use it on our baseline test, so these delays might net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide all variables for this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_records = 100000\n",
    "number_of_versions = 5\n",
    "number_of_fields = 11\n",
    "number_of_values_in_domain=20\n",
    "\n",
    "number_of_tests = 5\n",
    "confidence_interval = 0.95\n",
    "\n",
    "host = 'localhost'\n",
    "\n",
    "performance_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First scenario\n",
    "Inserting all records and adding the semantic operations afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = first_scenario()\n",
    "    performance_results = performance_results.append({'stage' : 'LoadingPhase', 'experiment': 'insert-first','time':time_taken}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Scenario\n",
    "Loading records only after inserting semantic operations in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records.head(10), 'valid_from_date') #initial insert\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:          \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "\n",
    "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = second_scenario()\n",
    "    performance_results = performance_results.append({'stage' : 'LoadingPhase', 'experiment': 'operations-first','time':time_taken}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Scenario\n",
    "\n",
    "In a common database scenario, the records would be just inserted as they were generated (in raw format). User would have to deal with heterogeneity afterwards, in the querying fase. Therefore, for the loading phase, only generate raw records and bulk insert into the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    client = MongoClient(host)\n",
    "    client[d.database_name][d.collection_name].insert_many(d.records)\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = third_scenario()\n",
    "    performance_results = performance_results.append({'stage' : 'LoadingPhase', 'experiment': 'baseline','time':time_taken}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying tests\n",
    "\n",
    "For the query tests, it does not matter which database (from first loading phase or from the second) is used. Both of them posess the same number of records, fields and domain values. Let's now analyse statistics in six different scenarios, just as in YCDB benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_database_preinsert():\n",
    "    ### Generate database just as before\n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    \n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])  \n",
    "       \n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correctly compare performance between the developed system and the baseline (an ordinary document-oriented database), this test performs a query rewriting before starting the time counter. It is important to notice, however, this test compares time performance only, regardless of usability gains achieved by this system. This query rewriting would have to be manually performed by the user, which would potentially also spend time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_baseline(query, database_generator):\n",
    "    ors = list()\n",
    "\n",
    "    for operation in database_generator:\n",
    "        operationType, version_date, arguments = operation\n",
    "\n",
    "        if operationType == 'grouping':\n",
    "            pass\n",
    "        elif operationType == 'translation':\n",
    "            oldValue = arguments['from']\n",
    "            newValue = arguments['to']\n",
    "            field = arguments['field']\n",
    "\n",
    "            if field not in query:\n",
    "                continue\n",
    "            else:\n",
    "                ands = [{field : newValue}]\n",
    "                \n",
    "\n",
    "        else:\n",
    "            raise BaseException('OperationType unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_database_postinsert():\n",
    "    ### Generate database just as before\n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    \n",
    "    d.collection.insert_many_by_dataframe(records.head(10), 'valid_from_date') #initial insert\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:          \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "\n",
    "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')    \n",
    "    \n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_read_test(percent_of_update):\n",
    "    ### Generate database just as before    \n",
    "    d = generate_test_database_preinsert()   \n",
    "    original_records = d.records.copy()\n",
    "\n",
    "    updates = math.floor(100*percent_of_update)\n",
    "    reads = 100-updates\n",
    "\n",
    "    sequence = ([True]*updates)\n",
    "    sequence.extend([False]*reads)\n",
    "    random.shuffle(sequence)    \n",
    "\n",
    "    records = [d.generate_record() for i in range(updates)]\n",
    "    records_2 = records.copy()\n",
    "\n",
    "    queries = []   \n",
    "\n",
    "    for i in range(reads):        \n",
    "        field = (random.choice(d.fields))[0]\n",
    "        value = random.choice(d.field_domain[field])\n",
    "        queries.append({field:value})   \n",
    "\n",
    "    queries_2 = queries.copy()     \n",
    "\n",
    "    start = time.time()\n",
    "    for operation in sequence:             \n",
    "        if operation: # insert - Nos nossos casos de uso não faz muito sentido deleções e updates. \n",
    "            record = records.pop()\n",
    "            start_2 = time.time()\n",
    "            d.collection.insert_one(json.dumps(record, default=str),record['valid_from_date'])                        \n",
    "            end_2 = time.time()\n",
    "            #print('Insertion time:' + str(end_2-start_2))\n",
    "        else:\n",
    "            start_2 = time.time()\n",
    "            d.collection.find_many(queries.pop()) ##Nao to considerando a presença ou ausência de índices\n",
    "            end_2 = time.time()\n",
    "            #print('Query time:' + str(end_2-start_2))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Operations time: \"+ str(end-start))\n",
    "    ##Evolute everything in the end            \n",
    "        \n",
    "    end = time.time()\n",
    "    preinsered_time = end-start\n",
    "    d.destroy()\n",
    "\n",
    "    client = MongoClient(host)        \n",
    "    db = client[d.database_name]\n",
    "    base_collection = db[d.collection_name]\n",
    "\n",
    "    base_collection.insert_many(original_records)\n",
    "\n",
    "    start = time.time()    \n",
    "    for operation in sequence:             \n",
    "        if operation: \n",
    "            record = records_2.pop()\n",
    "            base_collection.insert_one(record)                      \n",
    "        else:\n",
    "            base_collection.find(queries_2.pop()) ##Isso nao faz exatamente sentido. Deveria gerar uma nova query \n",
    "    end = time.time()    \n",
    "    baseline_time = (end-start)\n",
    "    client.drop_database(d.database_name)\n",
    "    \n",
    "    return ({'preinserted': preinsered_time, 'baseline': baseline_time})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 0 - Full Insertion - Lazy insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/Collection.py:59: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  normalized = json_normalize(self.collection_versions.find())\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/Collection.py:59: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  normalized = json_normalize(self.collection_versions.find())\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['previous_operation.field'],None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/GroupingOperation.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations time: 1.4635465145111084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/Collection.py:59: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  normalized = json_normalize(self.collection_versions.find())\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['previous_operation.field'],None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/GroupingOperation.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations time: 1.3101401329040527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/Collection.py:59: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  normalized = json_normalize(self.collection_versions.find())\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['previous_operation.field'],None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations time: 1.460247278213501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/Collection.py:59: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  normalized = json_normalize(self.collection_versions.find())\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/GroupingOperation.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['previous_operation.field'],None), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations time: 1.2835564613342285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/Collection.py:59: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  normalized = json_normalize(self.collection_versions.find())\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['previous_operation.field'],None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/TranslationOperation.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n",
      "/home/pedro/Documents/USP/Mestrado/Pesquisa/mongo-test/semantic_heterogeneous_database/GroupingOperation.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  versions_df_p['field_value'] = versions_df_p.apply(lambda row: Document.get(row['next_operation.field'], None), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations time: 1.608445405960083\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(1)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>experiment</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.049211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>insert-first</td>\n",
       "      <td>1.463605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.049995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>insert-first</td>\n",
       "      <td>1.310198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.047019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>insert-first</td>\n",
       "      <td>1.460304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.041640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>insert-first</td>\n",
       "      <td>1.283629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.040634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Full Insertion</td>\n",
       "      <td>insert-first</td>\n",
       "      <td>1.608504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stage    experiment      time\n",
       "0  Full Insertion      baseline  0.049211\n",
       "1  Full Insertion  insert-first  1.463605\n",
       "2  Full Insertion      baseline  0.049995\n",
       "3  Full Insertion  insert-first  1.310198\n",
       "4  Full Insertion      baseline  0.047019\n",
       "5  Full Insertion  insert-first  1.460304\n",
       "6  Full Insertion      baseline  0.041640\n",
       "7  Full Insertion  insert-first  1.283629\n",
       "8  Full Insertion      baseline  0.040634\n",
       "9  Full Insertion  insert-first  1.608504"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 0 - Full Insertion - Not lazy insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(1,False)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion Nonlazy', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'Full Insertion Nonlazy', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1 - 50/50\n",
    "\n",
    "In the first update scenario, a workload of 50% of reads and 50% of writes.\n",
    "\n",
    "Note there is a difference between the test using the first generation method and the second one, due to lazy records evolution adopted in the prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(0.5)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Insertion (no query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(1)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : '50/50', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2 - Read Heavy\n",
    "\n",
    "In this scenario, 95% of operations executed are read operations, while 5% are write operations (insertion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(0.05)\n",
    "    performance_results = performance_results.append({'stage' : 'ReadHeavy', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'ReadHeavy', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3 - WriteHeavy\n",
    "\n",
    "In this scenario, 95% of operations executed are write operations, while 5% are read operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(0.95)\n",
    "    performance_results = performance_results.append({'stage' : 'WriteHeavy', 'experiment': 'baseline','time':time_taken['baseline']}, ignore_index=True)\n",
    "    performance_results = performance_results.append({'stage' : 'WriteHeavy', 'experiment': 'insert-first','time':time_taken['preinserted']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
