{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Heterogeneous Database Simulations\n",
    "\n",
    "Let's start generating random records and semantic operations, which will be used to execute performance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from database_generator import DatabaseGenerator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Phase\n",
    "\n",
    "Inserting all records generated in the semantic heterogeneous database. Please note PyMongo library may diminish performance of insertions. However, because the simulator internally uses it, it is fair to also use it on our baseline test, so these delays might net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide all variables for this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_records = 100000\n",
    "number_of_versions = 5\n",
    "number_of_fields = 11\n",
    "number_of_values_in_domain=20\n",
    "\n",
    "number_of_tests = 5\n",
    "confidence_interval = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First scenario\n",
    "Inserting all records and adding the semantic operations afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_scenario_times = list()\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    time_taken = first_scenario()\n",
    "    first_scenario_times.append(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Scenario\n",
    "Loading records only after inserting semantic operations in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records.head(10), 'valid_from_date') #initial insert\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:          \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "\n",
    "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('grouping', datetime.datetime(2053, 1, 13, 0, 0), {'fieldName': 'okrpm', 'oldValues': [3381864, 7431303], 'newValue': 9019951})\n",
      "('grouping', datetime.datetime(2082, 8, 24, 0, 0), {'fieldName': 'jkovi', 'oldValues': [2446727, 3037643], 'newValue': 7654390})\n",
      "('translation', datetime.datetime(2028, 10, 27, 0, 0), {'fieldName': 'tonfs', 'oldValue': '2027-07-22', 'newValue': '2054-08-14'})\n",
      "('grouping', datetime.datetime(2096, 10, 6, 0, 0), {'fieldName': 'zrday', 'oldValues': ['wgkjjtrcnjsheueysanealcuznvxquihsu', 'xdtvnzrixkcbubihuzsrvdwidgdfswnfwr'], 'newValue': 'gqbdvpmegxsxkbcivibeskqanoxgsqzfdh'})\n",
      "('translation', datetime.datetime(2094, 11, 12, 0, 0), {'fieldName': 'yuxox', 'oldValue': 'ihkkvvpafnobvcqvpkrybcffuookpjouxe', 'newValue': 'rzjfafaobcvcznluhcabjmyshvflaskyxw'})\n",
      "('grouping', datetime.datetime(2034, 9, 5, 0, 0), {'fieldName': 'zpzbk', 'oldValues': ['2044-02-14', '2035-10-17'], 'newValue': '2094-03-06'})\n",
      "('grouping', datetime.datetime(2082, 12, 23, 0, 0), {'fieldName': 'zpzbk', 'oldValues': ['2033-01-21', '2077-02-02'], 'newValue': '2094-03-06'})\n",
      "('grouping', datetime.datetime(2057, 4, 2, 0, 0), {'fieldName': 'coqvv', 'oldValues': [6066396, 3632555], 'newValue': 9425257})\n",
      "('grouping', datetime.datetime(2071, 5, 8, 0, 0), {'fieldName': 'yxdeg', 'oldValues': [7273946, 7458701], 'newValue': 715283})\n",
      "('translation', datetime.datetime(2060, 10, 20, 0, 0), {'fieldName': 'pmuqj', 'oldValue': '2045-11-18', 'newValue': '2026-10-26'})\n",
      "('grouping', datetime.datetime(2061, 5, 22, 0, 0), {'fieldName': 'ittad', 'oldValues': ['2016-04-21', '2077-09-19'], 'newValue': '2020-02-03'})\n",
      "('grouping', datetime.datetime(2003, 2, 14, 0, 0), {'fieldName': 'yxdeg', 'oldValues': [7273946, 5061883], 'newValue': 7172948})\n",
      "('translation', datetime.datetime(2002, 12, 21, 0, 0), {'fieldName': 'kgrui', 'oldValue': '2023-01-22', 'newValue': '2050-02-01'})\n",
      "('grouping', datetime.datetime(2018, 12, 10, 0, 0), {'fieldName': 'nnwgs', 'oldValues': [3843449, 5622147], 'newValue': 5294142})\n",
      "('translation', datetime.datetime(2056, 3, 26, 0, 0), {'fieldName': 'qrueh', 'oldValue': 'wrjgmiykztafbnmpdxlooccbkldnkstvxj', 'newValue': 'xetuzznzyhfmkmzqbokrrgysibxhkjdvaj'})\n",
      "('grouping', datetime.datetime(2088, 10, 11, 0, 0), {'fieldName': 'kgrui', 'oldValues': ['2018-12-24', '2047-02-07'], 'newValue': '2032-05-25'})\n",
      "('grouping', datetime.datetime(2037, 8, 19, 0, 0), {'fieldName': 'edknz', 'oldValues': ['2060-05-18', '2030-09-07'], 'newValue': '2060-05-18'})\n",
      "('translation', datetime.datetime(2075, 10, 23, 0, 0), {'fieldName': 'garyl', 'oldValue': 'loqwnbtmpdrprythoxqnxneggfwygzdfav', 'newValue': 'yvbjrzzuddcrrlbcjkzymczduysfbsuxft'})\n",
      "('grouping', datetime.datetime(2038, 10, 24, 0, 0), {'fieldName': 'biwmr', 'oldValues': [8450867, 2840437], 'newValue': 3331525})\n",
      "('grouping', datetime.datetime(2026, 7, 13, 0, 0), {'fieldName': 'dkjlv', 'oldValues': ['zmgfazvujrvcaomomujzfgmezzoodkodxh', 'qvukgxuqhdbgeutqrbnmqgrorhbgvubbnn'], 'newValue': 'mgwejzcgkhocdraqmzvxzdtafesbfsgauf'})\n"
     ]
    }
   ],
   "source": [
    "second_scenario_times = list()\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    time_taken = second_scenario()\n",
    "    second_scenario_times.append(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Scenario\n",
    "\n",
    "In a common database scenario, the records would be just inserted as they were generated (in raw format). User would have to deal with heterogeneity afterwards, in the querying fase. Therefore, for the loading phase, only generate raw records and bulk insert into the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date') #initial insert    \n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_scenario_times = list()\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    time_taken = third_scenario()\n",
    "    third_scenario_times.append(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying tests\n",
    "\n",
    "For the query tests, it does not matter which database (from first loading phase or from the second) is used. Both of them posess the same number of records, fields and domain values. Let's now analyse statistics in six different scenarios, just as in YCDB benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1 - Writes Heavy\n",
    "\n",
    "In the first update scenario, a workload of 50% of reads and 50% of writes.\n",
    "\n",
    "Note there is a difference between the test using the first generation method and the second one, due to lazy insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_database():\n",
    "    ### Generate database just as before\n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    \n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])  \n",
    "       \n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_heavy(percent_of_update):\n",
    "    ### Generate database just as before\n",
    "    d = generate_test_database()   \n",
    "\n",
    "    ##Pensar numa maneira melhor de determinar essa ordem\n",
    "    sequence = [random.random() > percent_of_update for i in range(10000)] #if true, update, if false, read\n",
    "\n",
    "    start = time.time()\n",
    "    for operation in sequence:\n",
    "        if operation:\n",
    "            pass # inserir\n",
    "        else:\n",
    "            pass #update \n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    d.destroy()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
