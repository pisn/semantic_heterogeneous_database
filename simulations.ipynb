{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Heterogeneous Database Simulations\n",
    "\n",
    "Let's start generating random records and semantic operations, which will be used to execute performance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from database_generator import DatabaseGenerator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Phase\n",
    "\n",
    "Inserting all records generated in the semantic heterogeneous database. Please note PyMongo library may diminish performance of insertions. However, because the simulator internally uses it, it is fair to also use it on our baseline test, so these delays might net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide all variables for this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_records = 100000\n",
    "number_of_versions = 5\n",
    "number_of_fields = 11\n",
    "number_of_values_in_domain=20\n",
    "\n",
    "number_of_tests = 5\n",
    "confidence_interval = 0.95\n",
    "\n",
    "host = 'localhost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First scenario\n",
    "Inserting all records and adding the semantic operations afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_scenario_times = list()\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    time_taken = first_scenario()\n",
    "    first_scenario_times.append(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Scenario\n",
    "Loading records only after inserting semantic operations in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records.head(10), 'valid_from_date') #initial insert\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:          \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "\n",
    "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')\n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_scenario_times = list()\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    time_taken = second_scenario()\n",
    "    second_scenario_times.append(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Scenario\n",
    "\n",
    "In a common database scenario, the records would be just inserted as they were generated (in raw format). User would have to deal with heterogeneity afterwards, in the querying fase. Therefore, for the loading phase, only generate raw records and bulk insert into the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_scenario():    \n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    start = time.time()\n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date') #initial insert    \n",
    "    \n",
    "    end = time.time()\n",
    "    d.destroy()\n",
    "    return (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_scenario_times = list()\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    time_taken = third_scenario()\n",
    "    third_scenario_times.append(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying tests\n",
    "\n",
    "For the query tests, it does not matter which database (from first loading phase or from the second) is used. Both of them posess the same number of records, fields and domain values. Let's now analyse statistics in six different scenarios, just as in YCDB benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_database_preinsert():\n",
    "    ### Generate database just as before\n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    \n",
    "    d.collection.insert_many_by_dataframe(records, 'valid_from_date')\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:    \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])  \n",
    "       \n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_database_postinsert():\n",
    "    ### Generate database just as before\n",
    "    d = DatabaseGenerator()\n",
    "    d.generate(number_of_records=number_of_records, number_of_versions=1, number_of_fields=number_of_fields,number_of_values_in_domain=number_of_values_in_domain)\n",
    "    records = pd.DataFrame(d.records)\n",
    "\n",
    "    \n",
    "    d.collection.insert_many_by_dataframe(records.head(10), 'valid_from_date') #initial insert\n",
    "\n",
    "    for i in range(4):\n",
    "        d.generate_version()        \n",
    "    \n",
    "    for operation in d.operations:          \n",
    "        d.collection.execute_operation(operation[0],operation[1],operation[2])\n",
    "\n",
    "    d.collection.insert_many_by_dataframe(records.head(-10), 'valid_from_date')    \n",
    "    \n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_read_test(percent_of_update):\n",
    "    ### Generate database just as before    \n",
    "    d = generate_test_database_preinsert()   \n",
    "    original_records = d.records.copy()\n",
    "\n",
    "    updates = math.floor(100*percent_of_update)\n",
    "    reads = 100-updates\n",
    "\n",
    "    sequence = ([True]*updates)\n",
    "    sequence.extend([False]*reads)\n",
    "    random.shuffle(sequence)    \n",
    "\n",
    "    records = [d.generate_record() for i in range(updates)]\n",
    "    records_2 = records.copy()\n",
    "\n",
    "    queries = []   \n",
    "\n",
    "    for i in range(reads):        \n",
    "        field = (random.choice(d.fields))[0]\n",
    "        value = random.choice(d.field_domain[field])\n",
    "        queries.append({field:value})   \n",
    "\n",
    "    queries_2 = queries.copy()     \n",
    "\n",
    "    start = time.time()\n",
    "    for operation in sequence:             \n",
    "        if operation: # insert - Nos nossos casos de uso não faz muito sentido deleções e updates. \n",
    "            record = records.pop()\n",
    "            d.collection.insert_one(json.dumps(record, default=str),record['valid_from_date'])            \n",
    "        else:\n",
    "            d.collection.find_many(queries.pop()) ##Nao to considerando a presença ou ausência de índices\n",
    "        \n",
    "    end = time.time()\n",
    "    preinsered_time = end-start\n",
    "    d.destroy()\n",
    "\n",
    "    client = MongoClient(host)        \n",
    "    db = client[d.database_name]\n",
    "    base_collection = db[d.collection_name]\n",
    "\n",
    "    base_collection.insert_many(original_records)\n",
    "\n",
    "    start = time.time()    \n",
    "    for operation in sequence:             \n",
    "        if operation: \n",
    "            record = records_2.pop()\n",
    "            base_collection.insert_one(record)            \n",
    "        else:\n",
    "            base_collection.find(queries_2.pop()) ##Isso nao faz exatamente sentido. Deveria gerar uma nova query \n",
    "    end = time.time()    \n",
    "    baseline_time = (end-start)\n",
    "    \n",
    "    return ({'preinserted': preinsered_time, 'baseline': baseline_time})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1 - Writes Heavy\n",
    "\n",
    "In the first update scenario, a workload of 50% of reads and 50% of writes.\n",
    "\n",
    "Note there is a difference between the test using the first generation method and the second one, due to lazy insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'preinserted': -16.288459062576294, 'baseline': 0.025977373123168945},\n",
       " {'preinserted': -13.573254346847534, 'baseline': 0.027072668075561523},\n",
       " {'preinserted': -22.292099475860596, 'baseline': 0.021023035049438477},\n",
       " {'preinserted': -21.198261499404907, 'baseline': 0.025505781173706055},\n",
       " {'preinserted': -14.858585119247437, 'baseline': 0.026025056838989258}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_heavy_times = list()\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    time_taken = update_and_read_test(0.5)\n",
    "    write_heavy_times.append(time_taken)\n",
    "\n",
    "write_heavy_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
